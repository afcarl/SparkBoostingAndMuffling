{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import sys,os,pickle\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "from BoostStumps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  1  2  3  4  5  6  7  8  9 \n",
      " 0  0  0  0  0  0  0  0  0  0  0 \n",
      " 1  0  0  0  0  1  1  1  0  0  0 \n",
      " 2  0  0  0  0  1  1  1  0  0  0 \n",
      " 3  0  0  0  0  1  1  1  0  0  0 \n",
      " 4  0  0  0  0  0  0  0  0  0  0 \n",
      " 5  0  0  0  0  0  0  0  0  0  0 \n",
      " 6  0  0  0  0  0  0  0  0  0  0 \n",
      " 7  0  0  0  0  0  0  0  0  0  0 \n",
      " 8  0  0  0  0  0  0  0  0  0  0 \n",
      " 9  0  0  0  0  0  0  0  0  0  0 \n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "#sc=SparkContext()\n",
    "\n",
    "from numpy.random import rand\n",
    "p=1\n",
    "data=[]\n",
    "size=10\n",
    "print '   '+''.join([\"%2.0f \"%i for i in range(size)])\n",
    "for i in range(size):\n",
    "    print \"%2.0f \"%i,\n",
    "    for j in range(size):\n",
    "        if np.abs(i-size/4)<size/5 and np.abs(j-size/2)<size/5:\n",
    "            y=2*(rand()<p)-1\n",
    "        else:\n",
    "            y=2*(rand()>p)-1\n",
    "        print \"%1.0f \"%((1+y)/2),\n",
    "        data.append(LabeledPoint(y,[i,j]))\n",
    "    print"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "#sc=SparkContext()\n",
    "\n",
    "from numpy.random import rand\n",
    "p=0.9\n",
    "data=[]\n",
    "size=100\n",
    "print ''.join([\"%2.0f \"%i for i in range(size)])\n",
    "print '',\n",
    "for j in range(size):\n",
    "    if np.abs(j-size/4)<size/10:\n",
    "        y=2*(rand()<p)-1\n",
    "    else:\n",
    "        y=2*(rand()>p)-1\n",
    "    print \"%1.0f \"%((1+y)/2),\n",
    "    data.append(LabeledPoint(y,[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRDD=sc.parallelize(data,numSlices=2)\n",
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# booster currently works only for binary data, choose label of interest\n",
    "Label=2\n",
    "dataRDD = sc.textFile(\"../data/covtype.data\")\\\n",
    "            .map(lambda line: [float(x.strip()) for x in line.split(',')])\\\n",
    "            .map(lambda line: LabeledPoint(1*(line[-1]==Label),line[:-1])).cache()\n",
    "#dataRDD = dataRDD.sample(False,0.1, seed=255).cache()\n",
    "dataRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x106df8ed0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features= 2 number of partitions= 2\n",
      "Sizes: Data1=100, trainingData=66, testData=34\n",
      "number of elements in GR= 2\n",
      "number of elements in GTR= 2\n",
      "GR no of partitions 2\n",
      "number of partitions in PS= 2\n"
     ]
    }
   ],
   "source": [
    "booster=Booster(sc,dataRDD,no_of_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print booster.report_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0  ,iteration:  0  ,split index:  1   ,gamma:  0.5  ,bound:  0.606530659713\n",
      "Training Error: 0  ,iteration:  1  ,split index:  0   ,gamma:  0.5  ,bound:  0.367879441171\n",
      "Training Error: 0  ,iteration:  2  ,split index:  0   ,gamma:  0.5  ,bound:  0.223130160148\n",
      "Training Error: 0  ,iteration:  3  ,split index:  0   ,gamma:  0.5  ,bound:  0.135335283237\n",
      "Training Error: 0  ,iteration:  4  ,split index:  0   ,gamma:  0.5  ,bound:  0.0820849986239\n",
      "Training Error: 0  ,iteration:  5  ,split index:  0   ,gamma:  0.5  ,bound:  0.0497870683679\n",
      "Training Error: 0  ,iteration:  6  ,split index:  0   ,gamma:  0.5  ,bound:  0.0301973834223\n",
      "Training Error: 0  ,iteration:  7  ,split index:  0   ,gamma:  0.5  ,bound:  0.0183156388887\n",
      "Training Error: 0  ,iteration:  8  ,split index:  0   ,gamma:  0.5  ,bound:  0.0111089965382\n",
      "Training Error: 0  ,iteration:  9  ,split index:  0   ,gamma:  0.5  ,bound:  0.00673794699909\n"
     ]
    }
   ],
   "source": [
    "Scores=[]\n",
    "for i in range(10):\n",
    "    booster.boosting_iteration()\n",
    "    Scores.append(booster.compute_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print booster.report_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- 0 1\n",
      "1 {'SS': array([ 1.        ,  1.        ,  0.67741935,  0.48387097,  0.48387097,\n",
      "        0.29032258,  0.16129032,  0.16129032, -0.09677419, -0.29032258,\n",
      "       -0.29032258, -0.5483871 , -0.5483871 , -0.74193548, -0.87096774, -1.        ]), 'Correlation': 1.0, 'Threshold': 0.0, 'alpha': inf, 'Feature_index': 1, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 1 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 2 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 3 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 4 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 5 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 6 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 7 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 8 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n",
      "-------------------------------------------------- 9 0\n",
      "corr of prev best nan\n",
      "0 {'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "        nan,  nan,  nan,  nan,  nan,  nan]), 'Correlation': nan, 'Threshold': 0.0, 'alpha': nan, 'Feature_index': 0, 'Threshold_index': 0}\n"
     ]
    }
   ],
   "source": [
    "prev_best=-1\n",
    "for i in range(len(booster.proposals)):\n",
    "    iteration_data=booster.proposals[i]\n",
    "    best_feature=iteration_data['best feature']\n",
    "    print '-'*50,i,best_feature\n",
    "    if prev_best>-1:\n",
    "        print 'corr of prev best',iteration_data['details'][prev_best]['SS'][prev_best_threshold_index]\n",
    "    print best_feature,iteration_data['details'][best_feature]\n",
    "    prev_best=best_feature\n",
    "    prev_best_threshold_index=iteration_data['details'][prev_best]['Threshold_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print booster.proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Correlation': 1.0,\n",
       "  'Feature_index': 1,\n",
       "  'SS': array([ 1.        ,  1.        ,  0.67741935,  0.48387097,  0.48387097,\n",
       "          0.29032258,  0.16129032,  0.16129032, -0.09677419, -0.29032258,\n",
       "         -0.29032258, -0.5483871 , -0.5483871 , -0.74193548, -0.87096774, -1.        ]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': inf},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan},\n",
       " {'Correlation': nan,\n",
       "  'Feature_index': 0,\n",
       "  'SS': array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan]),\n",
       "  'Threshold': 0.0,\n",
       "  'Threshold_index': 0,\n",
       "  'alpha': nan}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster.Strong_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainScores=[]\n",
    "for train,test in Scores:\n",
    "    trainScores.append(train.flatMap(lambda A:list(A)).collect())\n",
    "len(trainScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 65)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresArray=np.stack(trainScores)\n",
    "shape(scoresArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEH3JXNEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a43a250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(scoresArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a338c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(scoresArray[9,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoostStumps.py   BoostStumps.pyc  \u001b[34mMuffled-Learning\u001b[m\u001b[m \u001b[34mdocs\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../src/BoostStumps.py\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "from time import time\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"A simple service class to log run time and pretty-print it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.T=[]\n",
    "    def stamp(self,name):\n",
    "        self.T.append((name,time()))\n",
    "    def str(self):\n",
    "        T=self.T\n",
    "        return '\\n'.join(['%6.2f : %s'%(T[i+1][1]-T[i][1],T[i+1][0]) for i in range(len(T)-1)])\n",
    "\n",
    "class Booster:\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "    def __init__(self,sc,Data,no_of_bins=10):\n",
    "        \"\"\" Given an RDD with labeled Points, create the RDD of data structures used for boosting\n",
    "\n",
    "        :param sc: SparkContext\n",
    "        :param Data: The input RDD\n",
    "\n",
    "        :creates: the self data structures used for boosting :\n",
    "                  T: timestamps\n",
    "                  GR:  RDD for training set\n",
    "                  GTR: RDD for test set\n",
    "                  PS: an array of training set RDDs, one per boosting iteration.\n",
    "                  Splits: A list of numpy arrays, holding the split points\n",
    "                  Strong_Classifier: A list of weak classifiers.\n",
    "        \"\"\"\n",
    "        self.sc=sc \n",
    "        self.T=Timer()\n",
    "        self.T.stamp('Started')\n",
    "\n",
    "        X=Data.first()\n",
    "        self.feature_no=len(X.features)\n",
    "        feature_no=self.feature_no\n",
    "        partition_no=Data.getNumPartitions()\n",
    "        if partition_no != self.feature_no:\n",
    "            Data=Data.repartition(feature_no).cache()\n",
    "        print 'number of features=',self.feature_no,'number of partitions=',Data.getNumPartitions()\n",
    "\n",
    "        self.iteration=0\n",
    "\n",
    "        # Split data into training and test\n",
    "        (trainingData,testData)=Data.randomSplit([0.7,0.3])\n",
    "        print 'Sizes: Data1=%d, trainingData=%d, testData=%d'%\\\n",
    "            (Data.count(),trainingData.cache().count(),testData.cache().count())\n",
    "        self.T.stamp('Split into train and test')\n",
    "        # Glom each partition into a local array\n",
    "        G=trainingData.glom()\n",
    "        GTest=testData.glom()  \n",
    "        self.T.stamp('glom')\n",
    "\n",
    "        # Add an index to each partition to identify it.\n",
    "        def f(splitIndex, iterator): yield splitIndex,iterator.next()\n",
    "        GI=G.mapPartitionsWithIndex(f) # RDD containing training data (glomed and indexed)\n",
    "        GTI=GTest.mapPartitionsWithIndex(f) #  RDD containing test data (glomed and indexed)\n",
    "        self.T.stamp('add partition index')\n",
    "\n",
    "        self.Prepare_data_structure(GI,GTI)\n",
    "        self.find_splits(no_of_bins)\n",
    "        self.Add_Weak_structures()\n",
    "\n",
    "        self.Strong_Classifier=[]\n",
    "        self.proposals=[]\n",
    "        self.Training_Error=[] #added\n",
    "        self.gamma=[] #added\n",
    "        self.sum_gamma=0\n",
    "        self.T.stamp('Finished Initialization')\n",
    "\n",
    "    ######################################################################################\n",
    "\n",
    "    def report_times(self):\n",
    "        \"\"\"return a string reporting the time stamps, use with \"print\"\n",
    "\n",
    "        :returns: the time stamps\n",
    "        :rtype: a string\n",
    "\n",
    "        \"\"\"\n",
    "        return self.T.str()\n",
    "\n",
    "    ######################################################################################\n",
    "    def Prepare_data_structure(self,GI,GTI):\n",
    "        \"\"\" Create a data structure for each glomed partition \n",
    "\n",
    "        :param GI: Glomed training set RDD\n",
    "        :param GTI: Glomed test set RDD\n",
    "\n",
    "        :creates: self.GR: RDD for training set (will have weak matrix added)\n",
    "                  self.GTR: RDD for test set (will be used as is to compute test error).\n",
    "\n",
    "        \"\"\"\n",
    "        def Prepare_partition_data_structure(A):\n",
    "            \"\"\"\n",
    "            :param A: A glomed partition\n",
    "            :returns: A partition data structure (missing the weak matrix which is added later)\n",
    "\n",
    "                    'index':The partition index (used to differentiate btwn partitions)\n",
    "                    'labels': An array of labels (-1/+1) one for each example\n",
    "                    'weights': An array of weights. one for each example\n",
    "                    'feature_values': A matrix of feature values (shape is [feature_no,rows])\n",
    "\n",
    "            :rtype: dict\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            rows=len(A[1])\n",
    "\n",
    "            columns=np.empty([feature_no,rows])\n",
    "            columns[:]=np.NaN\n",
    "            print 'Prepare_partition_data_structure',feature_no,np.shape(columns)\n",
    "\n",
    "            labels=np.empty(rows)\n",
    "            labels[:]=np.NaN\n",
    "\n",
    "            for j in range(rows):\n",
    "                LP=A[1][j]\n",
    "                labels[j]=LP.label\n",
    "                for i in range(feature_no):\n",
    "                    columns[i,j]=LP.features[i]\n",
    "            return {'index':A[0],\\\n",
    "                    'labels':labels,\\\n",
    "                    'weights':np.ones(len(labels)),\\\n",
    "                    'feature_values':columns}\n",
    "        #=========================================================================\n",
    "\n",
    "        feature_no=self.feature_no\n",
    "\n",
    "        # Prepare the train and test data structures for each partition.\n",
    "        self.GR=GI.map(Prepare_partition_data_structure)\n",
    "        self.GTR=GTI.map(Prepare_partition_data_structure)\n",
    "\n",
    "        print 'number of elements in GR=', self.GR.cache().count()\n",
    "        print 'number of elements in GTR=', self.GTR.cache().count()\n",
    "        self.T.stamp('Prepare_partition_data_structure')\n",
    "\n",
    "\n",
    "  \n",
    "    ##############################################################################\n",
    "    def find_splits(self,number_of_bins=10):\n",
    "        \"\"\"Compute the split points for each feature to create number_of_bins bins\n",
    "\n",
    "        :param number_of_bins: number of bins desired (default 10)\n",
    "        \n",
    "        :creates self.Splits: list of no_features numpy arrays, each\n",
    "                              containing the number_of_bins+1 split\n",
    "                              points (the last one is \"infinity\")\n",
    "        \"\"\"\n",
    "        def find_split_points(A):\n",
    "            \"\"\" A partition task: find the split points for a single feture whose index\n",
    "            is A['index']%feature_no\n",
    "\n",
    "            :param A: The partition data structure\n",
    "            :returns: (feature index, split points)\n",
    "            :rtype: tuple\n",
    "\n",
    "            \"\"\"\n",
    "            j=A['index'] % feature_no\n",
    "            S=np.sort(A['feature_values'][j,:])\n",
    "            L=len(S) \n",
    "            step=int(np.ceil(float(L)/number_of_bins))\n",
    "            return (j,S[range(step,L,step)])\n",
    "\n",
    "        #=========================================================================\n",
    "        #print 'no of bins', number_of_bins\n",
    "        GR=self.GR\n",
    "        feature_no=self.feature_no\n",
    "        partition_no=GR.getNumPartitions()\n",
    "        print 'GR no of partitions', partition_no\n",
    "        Splits=GR.map(find_split_points).collect()\n",
    "        #Splits=[]\n",
    "        #for A in GR.collect():\n",
    "        #    Splits.append(find_split_points(A))\n",
    "\n",
    "        max_no=np.array([np.finfo(float).max]) # max_no is the maximal value represented by a float\n",
    "\n",
    "        # Average the split points across the partitions corresponding to the same feature.\n",
    "        Splits1=[]\n",
    "        for i in range(feature_no):\n",
    "            S=Splits[i][1]\n",
    "            n=1  # number of copies (for averaging)\n",
    "            j=i+feature_no\n",
    "            while j<partition_no:\n",
    "                S+=Splits[j][1]\n",
    "                n+=1.0\n",
    "                j+=feature_no\n",
    "            Splits1.append(np.concatenate([S/n,max_no]))\n",
    "\n",
    "        self.Splits=Splits1   # store split points array in self.Splits\n",
    "        self.T.stamp('Compute Split points')\n",
    "\n",
    "    #############################################################\n",
    "    def Add_Weak_structures(self):\n",
    "        \"\"\"Create matrix for each partition to facilitate finding the weighted errors\n",
    "        of the weak rules using a single matrix multiplication\n",
    "        \"\"\"\n",
    "        def Add_weak_learner_matrix(A):\n",
    "            \"\"\" This procedure adds to the partition data structure the weak-rule error matrix\n",
    "\n",
    "            :param A: The partition data structure \n",
    "\n",
    "            :returns: input A with added field 'M'\n",
    "            :rtype: dict\n",
    "            \"\"\"\n",
    "\n",
    "            index=A['index']%feature_no\n",
    "            Splits=BC_Splits.value[index]\n",
    "\n",
    "            Col=A['feature_values'][index,:]\n",
    "\n",
    "            ### The matrix M is organized as follows: \n",
    "            # * There are as many rows as there are thresholds in Splits (last one is inf)\n",
    "            # * There are as many columns as there are examples in this partition.\n",
    "            # For threshold i, the i'th rw of M is +1 \n",
    "            #     if Col is smaller than the trehold Splits[i] and -1 otherwise\n",
    "\n",
    "            M=np.empty([len(Splits),len(Col)])\n",
    "            M[:]=np.NaN\n",
    "\n",
    "            for i in range(len(Splits)):\n",
    "                M[i,:]=2*(Col<Splits[i])-1\n",
    "\n",
    "            A['M']=M # add M matrix to the data structure.\n",
    "            return A\n",
    "        #=========================================================================\n",
    "\n",
    "        BC_Splits=self.sc.broadcast(self.Splits) #broadcast split points\n",
    "\n",
    "        feature_no=self.feature_no\n",
    "\n",
    "        self.PS=[None]\n",
    "        \n",
    "        self.PS[0]=self.GR.map(Add_weak_learner_matrix)\n",
    "        #L=[]\n",
    "        #for A in self.GR.collect():\n",
    "        #    L.append(Add_weak_learner_matrix(A))\n",
    "        #self.PS[0]=self.sc.parallelize(L)\n",
    "\n",
    "        print 'number of partitions in PS=',self.PS[0].cache().count()\n",
    "        self.T.stamp('Add_weak_learner_matrix')\n",
    "\n",
    "    #############################################################\n",
    "    def boosting_iteration(self):\n",
    "        \"\"\"perform one boosting iteration. Consisting of finding the lowest\n",
    "        error weak rule , adding it, and updating the weights\n",
    "\n",
    "        :updates: PS: appends a new RDD (all is the same other than the weight vector) to the end of the list\n",
    "                  \n",
    "        \"\"\"\n",
    "        def Find_weak(A):\n",
    "            \"\"\"Find the best split for a single feature on a single partition\n",
    "\n",
    "            :param A: Partition data structure\n",
    "\n",
    "            :returns: a dict describing the added weak classifier\n",
    "                    'Feature_index':   the index of the best feature\n",
    "                    'Threshold_index': the index of the best threshold (the split point)\n",
    "                    'Threshold':       the value of the best threshold \n",
    "                    'Correlation':     the weighted correlation of the best weak rule\n",
    "                    'SS':              the weighted correlations of all of the split points.\n",
    "            :rtype: dict\n",
    "            \"\"\"\n",
    "            index=A['index']%feature_no\n",
    "            Splits=BC_Splits.value[index]\n",
    "\n",
    "            M=A['M']\n",
    "            weights=A['weights']\n",
    "            weighted_Labels=weights*A['labels']\n",
    "            SS=np.dot(M,weighted_Labels)/np.sum(weights)\n",
    "            i_max=np.argmax(np.abs(SS))\n",
    "            return {'Feature_index':A['index']%feature_no,\\\n",
    "                    'Threshold_index':i_max,\\\n",
    "                    'Threshold':Splits[i_max],\\\n",
    "                    'Correlation':SS[i_max],\\\n",
    "                    'SS':SS\n",
    "                }\n",
    "\n",
    "        def update_weights(A):\n",
    "            \"\"\"Update the weights of the examples belonging to this partition\n",
    "\n",
    "            :param A: The partition data structure\n",
    "\n",
    "            :returns: A partition data structure with updated weights\n",
    "            :rtype: dict\n",
    "\n",
    "            \"\"\"\n",
    "            best_splitter=BC_best_splitter.value\n",
    "\n",
    "            F_index=best_splitter['Feature_index']\n",
    "            Thr=best_splitter['Threshold']\n",
    "            alpha=best_splitter['alpha']\n",
    "            y_hat=2*(A['feature_values'][F_index,:]<Thr)-1\n",
    "            y=A['labels']\n",
    "            weights=A['weights']*np.exp(-alpha*y_hat*y)\n",
    "            weights /= sum(weights)\n",
    "\n",
    "            A['weights']=weights\n",
    "            return A\n",
    "        \n",
    "        def find_Training_Error(A): ##added\n",
    "            #error=0\n",
    "            best_splitter=BC_best_splitter.value\n",
    "\n",
    "            F_index=best_splitter['Feature_index']\n",
    "            #print 'index' , F_index\n",
    "            Thr=best_splitter['Threshold']\n",
    "            #alpha=best_splitter['alpha']\n",
    "            y_hat=2*(A['feature_values'][F_index,:]<Thr)-1\n",
    "            y=A['labels']\n",
    "            weights=A['weights']\n",
    "            error=y_hat * y\n",
    "            error=(error * weights)/sum(weights)\n",
    "            error=error[error<0]\n",
    "            return -1*sum(error)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #=========================================================================\n",
    "\n",
    "        i=self.iteration\n",
    "\n",
    "        self.T.stamp('Start main loop %d'%i)\n",
    "\n",
    "        feature_no=self.feature_no\n",
    "\n",
    "        BC_Splits = self.sc.broadcast(self.Splits)\n",
    "        prop=self.PS[i].map(Find_weak).collect()\n",
    "\n",
    "        corrs=[p['Correlation'] for p in prop]\n",
    "        best_splitter_index=np.argmax(np.abs(corrs))\n",
    "        best_splitter = prop[best_splitter_index]\n",
    "        self.proposals.append({'iter ':i,'best feature':best_splitter_index,'details':prop})\n",
    "\n",
    "        corr=best_splitter['Correlation']\n",
    "        best_splitter['alpha']=0.5*np.log((1+corr)/(1-corr))\n",
    "\n",
    "        BC_best_splitter=self.sc.broadcast(best_splitter)\n",
    "        self.Strong_Classifier.append(best_splitter)\n",
    "        '''\n",
    "        adict=self.PS[i].collect()\n",
    "        a1=adict[best_splitter_index]\n",
    "        if(best_splitter_index==0):\n",
    "            c=1\n",
    "        else:\n",
    "            c=0\n",
    "        a2=adict[c]\n",
    "        print type(adict), type(a1),type(a2)\n",
    "        weights1=a1['weights']\n",
    "        weights2=a2['weights']\n",
    "        wtest1=weights1[weights1<0]\n",
    "        wtest2=weights2[weights2<0]\n",
    "        #print 'weights1 ', len(weights1), 'weights_test ', len(wtest1) ,'weights2 ', len(weights2), 'weights_test 2', len(wtest2)\n",
    "        #print 'final ', len(weights1)+len(weights2)\n",
    "        #print 'sum ', sum(weights1)+sum(weights2)\n",
    "        '''\n",
    "        \n",
    "        E=self.PS[i].map(find_Training_Error).collect()\n",
    "        trainingError=E[best_splitter_index]\n",
    "        \n",
    "        gamma_t=0.5-trainingError\n",
    "        self.Training_Error.append(trainingError)\n",
    "        \n",
    "        self.gamma.append(gamma_t)\n",
    "        \n",
    "        expo = self.sum_gamma + (gamma_t ** 2)\n",
    "        self.sum_gamma=expo\n",
    "        \n",
    "        bound=math.exp(-2 * expo)\n",
    "        \n",
    "        #bound=0\n",
    "        print \"Training Error:\", trainingError, ' ,iteration: ', self.iteration , ' ,split index: ' , best_splitter_index, '  ,gamma: ',gamma_t, \" ,bound: \",bound\n",
    "        \n",
    "        BC_Strong_Classifier=self.sc.broadcast(self.Strong_Classifier)\n",
    "        self.T.stamp('found best splitter %d'%i)\n",
    "\n",
    "        newPS=self.PS[i].map(update_weights).cache()\n",
    "        #L=[]\n",
    "        #for A in self.PS[i].collect():\n",
    "        #    L.append(update_weights(A))\n",
    "        #newPS=self.sc.parallelize(L).cache()\n",
    "        newPS.count()\n",
    "        self.PS.append(newPS)\n",
    "        \n",
    "        \n",
    "        self.T.stamp('Updated Weights %d'%i)\n",
    "        self.iteration+=1\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    def compute_scores(self):\n",
    "\n",
    "        def calc_scores(Strong_Classifier,Columns,Lbl):\n",
    "\n",
    "            Scores=np.zeros(len(Lbl))\n",
    "\n",
    "            for h in Strong_Classifier:\n",
    "                index=h['Feature_index']\n",
    "                Thr=h['Threshold']\n",
    "                alpha=h['alpha']\n",
    "                y_hat=2*(Columns[index,:]<Thr)-1\n",
    "                Scores += alpha*y_hat*Lbl\n",
    "            return Scores\n",
    "\n",
    "        def get_scores(A):\n",
    "            Strong_Classifier=BC_Strong_Classifier.value\n",
    "            Scores = calc_scores(Strong_Classifier,A['feature_values'],A['labels'])\n",
    "            return Scores\n",
    "        #=========================================================================\n",
    "        \n",
    "        train_scores=self.GR.map(get_scores)\n",
    "        test_scores=self.GTR.map(get_scores)\n",
    "        return train_scores,test_scores\n",
    "\n",
    "\n",
    "###################################################################\n",
    "def generate_data(sc):\n",
    "    p=0.9\n",
    "    data=[]\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if np.abs(i-4)<3 and np.abs(j-6)<3:\n",
    "                y=2*(rand()<p)-1\n",
    "            else:\n",
    "                y=2*(rand()>p)-1\n",
    "            print \"%1.0f \"%((1+y)/2),\n",
    "            data.append(LabeledPoint(y,[i,j]))\n",
    "        print\n",
    "\n",
    "\n",
    "    return sc.parallelize(data)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sc=SparkContext()\n",
    "    dataRDD = generate_data(sc)\n",
    "    booster=Booster(sc,dataRDD)\n",
    "    Scores=[]\n",
    "    for i in range(10):\n",
    "        booster.boosting_iteration()\n",
    "        Scores.append(booster.compute_scores())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
